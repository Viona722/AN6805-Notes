# 1. KNN classifier
1. 适用于 数值型数据，对分类和回归都可以使用，但对高维数据效果较差，高维会维度灾难，高维空间中距离度量变得无意义。
1. Inductive learning: Use f: x -> y
2. Lazy Learning: Find nearest or most likelihood record
3. Nearest-Neighbor Classifiers
   1. How to define what is close:
![alt text](image.png)
   Euclidean distance and Manhattan Distance are correlated.
   cosine are used in face recognization: you know what you want to match, better deal with scale
   1 and 3 most frequently.
   2. Need rescale to make every attributes in the same scale, or there might exists some attributes affect more
   3. use encode when number doesn't means big or small, use 1,2,... when diff category have relationships such as 收入高中低
   4. Important to choose right k because small k leads to sensitive to noise and bigger k leads to include points from other classes
   5. Determine Class lable
      1. majority vote: every neighbor has the same impact on the classification
      2. weighted voting scheme: weight = 1/D(x*,xi)^2
   6. How to Solve Scaling:
      1. Normalization: Min-max
      2. Normalization: Standardization, z-score
   7. Training can let we know the weight and best K，训练时间几乎为 0（无训练过程），预测时间非常高，每次预测都要计算所有样本的距离。
# 2. Decision Tree 
1. For Classification, 适用于 数值型和类别型数据，可处理缺失值，适用于分类和回归任务
2. There could be more than one tree that fits the same data!
3. Decision Tree Induction:
   1. Hunt's Algorithm：找到能完全分类的变量，
   2. CART
   3. ID#
   4. C4.5
   5. SLIQ
   6. SPRINT
