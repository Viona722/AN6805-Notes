# 1. KNN classifier
1. Inductive learning: Use f: x -> y
2. Lazy Learning: Find nearest or most likelihood record
3. Nearest-Neighbor Classifiers
   1. How to define what is close:
![alt text](image.png)
   Euclidean distance and Manhattan Distance are correlated.
   cosine are used in face recognization: you know what you want to match, better deal with scale
   1 and 3 most frequently.
   2. Need rescale to make every attributes in the same scale, or there might exists some attributes affect more
   3. use encode when number doesn't means big or small, use 1,2,... when diff category have relationships such as 收入高中低
   4. Important to choose right k because small k leads to sensitive to noise and bigger k leads to include points from other classes
   5. Determine Class lable
      1. majority vote: every neighbor has the same impact on the classification
      2. weighted voting scheme: weight = 1/D(x*,xi)^2
   6. How to Solve Scaling:
      1. Normalization: Min-max
      2. Normalization: Standardization, z-score
   7. Training can let we know the weight and best K
